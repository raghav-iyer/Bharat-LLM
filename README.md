Decoder Only Transformer with Multihead Self Attention, Rotatory positional embedding, sentence piece vocab, inbuilt tokenizer ,new architecture for LLM
